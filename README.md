# PromptPackr
**Compress your conversations. Reuse your prompts. Supercharge your agents.**  PromptPackr is a local, privacy-first tool that **compresses lengthy chat history into structured memory** â€” making it easier to reuse, audit, and rehydrate LLM prompts.   It's like creating `.zip` files for your AI conversations.

## âœ¨ Features
- ğŸ”¹ Generate **compressed summaries** of long conversations
- ğŸ§¾ Extract **key instructions and goals**
- ğŸ­ Detect **tone and communication style**
- ğŸ§  Enable **prompt rehydration** for autonomous agents or RAG pipelines
- ğŸ”’ 100% **offline** â€” no API keys or cloud calls

  ## ğŸ›  Built With
- ğŸ¦™ [Ollama](https://ollama.com/) â€” Local LLM runtime (e.g., Mistral, LLaMA 3)
- ğŸŒ [Streamlit](https://streamlit.io/) â€” Web-based UI
- ğŸ Python â€” Core compression logic

## ğŸ“¸ Demo
<img width="298" alt="res1" src="https://github.com/user-attachments/assets/d790bcf7-92cc-4967-a939-81ba37d29db0" />

## ğŸš€ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/promptpackr.git
cd promptpackr
```

2. Install dependencies
```bash
pip install -r requirements.txt
```

3. Install and run Ollama
Download Ollama:
ğŸ‘‰ https://ollama.com/download

Pull a model (e.g., Mistral):
```bash
ollama run mistral
```

4. Launch the app
```bash
streamlit run app.py
```

ğŸ§ª Sample Input
Just upload a .txt file of a multi-turn conversation:

ğŸ“¤ Output Format
ğŸ”¹ Summary â€” High-level chat overview
ğŸ§¾ Instructions â€” Clear, bullet-pointed tasks
ğŸ­ Tone â€” Communication style (e.g., friendly, formal)

All output is returned in structured format and can optionally be exported or rehydrated into a new prompt.

ğŸ“¦ Folder Structure
```bash
promptpackr/
â”œâ”€â”€ app.py                # Streamlit interface
â”œâ”€â”€ ollama_query.py       # LLM interaction logic
â”œâ”€â”€ prompts.py            # Prompt templates
â”œâ”€â”€ sample_chat.txt       # Sample input
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
ğŸ§  Use Cases
âœï¸ Improve prompt reusability
ğŸ¤– Store long LLM conversations in compressed form
ğŸ’¬ Build memory-aware agents
ğŸ§ª Analyze support logs or meeting transcripts

ğŸ” Local-Only Architecture
No API keys. No cloud servers.
PromptPackr runs 100% locally using Ollama + open-source LLMs.

.

ğŸ“¬ Contributing
Have a feature idea? Want to add Chainlit or memory export?
Pull requests are welcome! You can also open an issue for feedback or improvements.

ğŸ”— Connect
Built with Varshitha Yanamala during #30DaysOfGenAI






